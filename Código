import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Carregando o dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv"
columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms',
           'population', 'households', 'median_income', 'median_house_value']
data = pd.read_csv(url, names=columns)

# Exibindo as primeiras linhas para inspeção
data.head()

# Analisando dados ausentes
print(data.isnull().sum())

# Preenchendo valores ausentes com a mediana
data.fillna(data.median(), inplace=True)

# Separando variáveis independentes e dependentes
X = data.drop('median_house_value', axis=1)
y = data['median_house_value']

# Dividindo em treino e teste (80% para treino, 20% para teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalizando os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score

# Modelos de Regressão

# 1. Regressão Linear
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

# 2. Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train_scaled, y_train)

# 3. Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train_scaled, y_train)

# Validação cruzada para avaliação dos modelos
cv_lr = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
cv_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
cv_gb = cross_val_score(gb_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')

print(f"Linear Regression CV MSE: {np.mean(cv_lr)}")
print(f"Random Forest CV MSE: {np.mean(cv_rf)}")
print(f"Gradient Boosting CV MSE: {np.mean(cv_gb)}")

from sklearn.model_selection import GridSearchCV

# Parâmetros para otimização do Random Forest
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Parâmetros para otimização do Gradient Boosting
gb_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 5]
}

# GridSearch para Random Forest
rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=5, scoring='neg_mean_squared_error')
rf_grid_search.fit(X_train_scaled, y_train)

# GridSearch para Gradient Boosting
gb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, cv=5, scoring='neg_mean_squared_error')
gb_grid_search.fit(X_train_scaled, y_train)

print(f"Best Random Forest Params: {rf_grid_search.best_params_}")
print(f"Best Gradient Boosting Params: {gb_grid_search.best_params_}")

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Predições nos dados de teste
y_pred_lr = lr_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)
y_pred_gb = gb_model.predict(X_test_scaled)

# Avaliando o desempenho dos modelos
mse_lr = mean_squared_error(y_test, y_pred_lr)
mae_lr = mean_absolute_error(y_test, y_pred_lr)

mse_rf = mean_squared_error(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)

mse_gb = mean_squared_error(y_test, y_pred_gb)
mae_gb = mean_absolute_error(y_test, y_pred_gb)

# Comparação dos resultados
print("Linear Regression - MSE:", mse_lr, "MAE:", mae_lr)
print("Random Forest - MSE:", mse_rf, "MAE:", mae_rf)
print("Gradient Boosting - MSE:", mse_gb, "MAE:", mae_gb)
